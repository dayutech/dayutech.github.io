---
title: 机器学习-贝叶斯分类器
abbrlink: 8850ca58
date: 2026-01-07 11:29:32
tags:
- 贝叶斯分类器
categories:
- - 机器学习
top: 1005
mathjax: true
description: 本文记录了有关贝叶斯分类器有关的原理，来自西瓜书，一些自己的理解。
---
# 贝叶斯决策论
假设有$N$种肯能的类别标记，即$y={c_1,c_2...c_n}$，$\lambda_{ij}$表示将实际类别为$j$的样本分类为$i$所产生的损失。
基于后验概率可以产生一个将样本$x$分类为$c_i$的期望损失，即在样本$x$上的条件风险。  
$$
R(c_i\|x)=\sum_{j=1}^{N} \lambda_{ij}P(c_j\|x) \tag{1}
$$
这个公式首先是得到x为$c_j$的概率然后乘以将$x$分类为$i$类型的损失，因为有$N$种类别，所以进行累加得到将$j$分类错误将会产生的损失称为期望损失。  
这里有一个问题需要澄清知道只有在分类错误的时候才会产生实际的损失那么如果将$j$分类为$j$就不会产生损失，按照这个理论是不是只需要累加$N-1$次就可以了   
实际上这里确是加的$N$次，这里就体现到$\lambda$这个损失的作用了，当分类正确的时候设置损失为$0$即$\lambda_{jj}=0$即可。  
我们分类的目标是使得整体样本在分类完成后整体的期望损失最小化，即目标是：  
$$
min(E_x\[R(c\|x)])\tag{2}
$$
分类的结果受对$x$的分类方法影响，设分类准测$h:x-\>y$，即通过模型$h$对$x$进行分类，那么分类的目标就是求使得总体风险最小化的$h$：  
$$
h\*(x)=\underset{c \in y}{arg \space min \space R(c\|x)}\tag{3}
$$
这里$h\*(x)$是一个永远也达不到的理想值。这里的思想是既然我们要使整体风险最小化，那么只需要求一个$h\*(x)$使得针对每一个样本$x$都取得最小的期望损失即可，  
稍微一想这就是不可能的，当对某一个样本$x_1$求得最佳的$h$后，继续对第二个样本$x_2$求最佳的$h$使得期望损失最小，那么这个过程中对$h$参数的调整势必导致此时的$h$  
不再是针对$x_1$最优的$h$了，所以这里的$h\*(x)$是一个理论上的最优值，我们最终求得的$h(x)$只能无限的逼近$h\*(x)$而不会等于。  
这个$h\*$被称为贝叶斯最优分类器，与之对应的总体风险$R\*(h)$称为贝叶斯风险。  
我们将误判损失$\lambda_{ij}$写为  
$$
\lambda_{ij}=
\begin{cases}
0,\space\space\space\space if\space i=j \\\\
1,\space\space\space\space otherwise \tag{4}
\end{cases}
$$
即 当分类正确的时候损失为$0$，而分类错误是损失为$1$   
$$
R(c \| x)=\sum_{j=1,j \ne i}^{N} P(c_j\|x)= 1 - P(c \| x) \tag{5}
$$
也就是说条件风险等于$1$减去将$x$分类为$c$即分类正确的概率，所以分类目标由最小化分类错误期望损失变成了最大化分类正确率，即：  
$$
h\*(x)=\underset{c \in y}{arg \space max} P(h(x)\|x)\tag{6}
$$
$P(c\|x)$被称为后验概率，一般来说后验概率不好确定，所以我们根据贝叶斯公式对问题进行转化。  
$$
P(c\|x)=\frac{P(c)P(x\|c)}{P(x)}\tag{7}
$$
$P(x)$表示选中$x$的概率是一个与类别$c$无关的值，所以我们可以忽略这个值得到。  
$$
P(c\|x) \propto P(c)P(x\|c)\tag{8}
$$
$P(c)$表示$x$是$c$类别的概率，我们可以通过各类样本出现的频率来进行估计，即$P(c)$为$c$类别占样本总数的比例。   
$P(x\|c)$ 表示$c$类别中出现$x$的概率，这个值是$c$的所有属性的联合概率，即我们要将样本每一个属性值出现的概率进行联合求得。  
假设每一个样本由d个属性且每个属性都是二值的，那么可能的组合就是$2^d$，随着属性种类以及属性的取值增多这个结果的增长是极快的。  
而我们的样本总数往往不足以赶上这个数量，于是乎用频率来估计概率的方法就不适用了。
# 极大似然估计
极大似然估计是基于一个假设的，即类条件概率是具有某种确定的分布形式，可以用一个函数进行表达。那么就可以基于训练样本对概率分布的参数进行估计。关于类别$c$的条件概率$P(x\|c)$，假设其具有某种确定的形式并被某一个参数向量  
$\theta_c$唯一确定，那么我们的任务就是利用训练集$D$估计参数$\theta_c$，于是可以将条件概率记为$P(x\|\theta_c)$  
令$D_c$表示训练集$D$中$c$类样本的集合，假设这些样本独立同分布，则参数$\theta_c$对于数据集$D_c$的似然为  
$$
P(D_c\|\theta_c)=\prod_{x \in D_c}^{} P(x\|\theta_c)\tag{9}
$$
这个似然表示在确定参数$\theta_c$的条件下取得数据集$D_c$的可能性，所以其值是通过在确定$\theta_c$的情况下取得$x$的概率的累乘求得的。  
根据式（6）以及式（8）我们的任务是最大化$P(x\|c)$，又因为$P(x\|c)$被$\theta_c$确定那么我们的目标也就是求使得$D_c$被选出来的可能性最大的$\theta_c$记作$\hat{\theta_c}$  
式子（9）中的连乘是一堆小于1的小数的连乘，最终的取值非常小不方便阅读以及比较，所以往往采用对数似然的方法即：  
$$
LL(\theta_c)=\log P(D_c\|\theta_c)=\sum_{x \in D_c}^{} \log P(x\| \theta_c) \tag{10}
$$
上面式子里是将对数运算的乘法变成加法，方便计算。  
此时进行对参数$\theta_c$进行极大似然估计  
$$
\hat{\theta_c} =\underset{\theta_c}{arg \space max} \space LL(\theta_c) \tag{11}
$$
还记得之前的假设吗？假设类条件概率$P(x\|c)$服从某一种概率分布，不妨认为这个概率分布为正态分布，$x$是一个向量，所以这不仅仅是正态分布还是一个多元正态分布。    
那么此时$\theta_c$的取值就是样本均值以及协方差了。即$\mathbf{\mu}$与$\mathbf{\sigma}^2$。
对于多元正态分布有  
$$  
p(\mathbf{x} \mid \boldsymbol{\mu}_c, \boldsymbol{\Sigma}_c) = 
\frac{1}{\sqrt{(2\pi)^d \|\boldsymbol{\Sigma}_c\|}} 
\exp\left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu}_c)^\top \boldsymbol{\Sigma}_c^{-1} (\mathbf{x} - \boldsymbol{\mu}_c) \right)
$$  

取对数  
$$  
\log p(x \mid \mu_c, \Sigma_{c}) = -\frac{d}{2} \log(2\pi) - \frac{1}{2} \log\|\Sigma_c\| - \frac{1}{2}(x - \mu_c)^T \Sigma_c^{-1} (x - \mu_c)
$$   

所以总对数似然为：  
$$  
LL(\theta_c) = \sum_{x \in D_c} \left [ -\frac{d}{2} \log(2\pi) - \frac{1}{2} \log|\Sigma_c| - \frac{1}{2}(x - \mu_c)^T \Sigma_c^{-1} (x - \mu_c) \right ]
$$  

现在要最大化 $ LL(\theta_c) $，即对 $ \mu_c $ 和 $ \Sigma_c $ 分别求偏导。
固定 $ \Sigma_c $，对 $ \mu_c $ 求导：

$$
\frac{\partial LL}{\partial \mu_c} = \sum_{x \in D_c} \left\[ -\frac{1}{2} \cdot (-2) \Sigma_c^{-1} (x - \mu_c) \right] = \sum_{x \in D_c} \Sigma_c^{-1} (x - \mu_c)
$$

令导数为0：
$$
\sum_{x \in D_c} (x - \mu_c) = 0
\Rightarrow \sum_{x \in D_c} x = \|D_c\| \mu_c
\Rightarrow \mu_c = \frac{1}{\|D_c\|} \sum_{x \in D_c} x
$$


故
$$
\boxed{\hat{\mu_c} = \frac{1}{\|D_c\|} \sum_{x \in D_c} x}\tag{12}
$$


对 $ \Sigma_c $ 求导。  
对数似然中关于 $ \Sigma_c $ 的项：  
$$
LL = -\frac{\|D_c\|}{2} \log\|\Sigma_c\| - \frac{1}{2} \sum_{x \in D_c} (x - \mu_c)^T \Sigma_c^{-1} (x - \mu_c)
$$

对矩阵求导得：  
- $ \frac{\partial \log\|\Sigma_c\|}{\partial \Sigma_c} = \Sigma_c^{-1} $
- $ \frac{\partial (x - \mu_c)^T \Sigma_c^{-1} (x - \mu_c)}{\partial \Sigma_c} = -\Sigma_c^{-1} (x - \mu_c)(x - \mu_c)^T \Sigma_c^{-1} $

最终可得：  
$$
\frac{\partial LL}{\partial \Sigma_c} = -\frac{\|D_c\|}{2} \Sigma_c^{-1} + \frac{1}{2} \sum_{x \in D_c} \Sigma_c^{-1} (x - \mu_c)(x - \mu_c)^T \Sigma_c^{-1}
$$
令导数为0，两边乘 $ \Sigma_c $ 得：  
$$
\|D_c\| I = \sum_{x \in D_c} (x - \mu_c){(x - \mu_c)}^T  
\Rightarrow \hat{\Sigma_c} = \frac{1}{\|D_c\|} \sum_{x \in D_c} (x - \hat{\mu_c})(x - \hat{\mu_c})^T  
$$

故
$$
\boxed{ \hat{\sigma_c}^2 = \frac{1}{\|D_c\|} \sum_{x \in D_c} (x - \hat{\mu}_c)(x - \hat{\mu}_c)^T}\tag{13}
$$
需注意的是，这种参数化的方法虽能使类条件概率估计变得相对简单，但估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。  
在现实应用中，欲做出能较好地接近在真实分布的假设，往往需在一定程度上利用关于应用任务本身的经验知识，否则若仅凭“猜测”来假设概率分布形式，很可能产生误导性的结果。



