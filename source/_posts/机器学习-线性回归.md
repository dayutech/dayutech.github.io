---
title: 机器学习-线性回归
abbrlink: '19883263'
date: 2025-11-10 16:47:19
tags:
mathjax: true
top: 1000
categories: 
- - 机器学习
---
# 什么是线性回归
线性方程
$$
\hat{y_{i} } = w \cdot x_{i} + b \tag{1}
$$
# 线性回归的基本假设
## 线性性
线性回归模型的形式为：
$$
Y=\beta_0 + \beta_1X_1 +\beta_2X_2+\beta_3X_3+...+\beta_4X_4+\varepsilon 
$$
线性性指的是：模型对参数（即回归系数 $\beta_0,\beta_1...\beta_n$是线性的，而不是对自变量 $X$ 必须是线性的。
换句话说：
- 允许$X$出现非线性形式（如$X^2,\ln_{}{x},\sin x$等，这些分线性的X可以被看作一个整体，结果仍然是线性的，后面的多项式回归，注入$X^2$这种也被看作一个整体$X$，那么多项式回归就变成了多元回归），
- 这些非线性变换后的项仍以线性组合的方式出现在模型中。
为什么非得针对$\beta$必须是线性的，而不是针对$X$是线性的，这是因为在使用最小二乘法计算残差方程的时候的自变量是$\beta$，而$X$来自样本数据，是已知的常数量，此时$\beta$才是真正的解释变量  
$\beta$如果是形如$e ^{\beta} $类型的非线性形式，那么最小二乘法求解残差方程的最小值的时候就不能利用凸函数的性质直接对两个变量分别求偏导，使其等于零来得到全局的最小值，只能通过梯度下降法来逐渐逼近最小值   
而且还容易陷入到局部最小值的怪圈里，无法求得全局的最小值，即局部最优解。
从这一点出发，我们在解方程的时候其实是解的$\beta$的值，所以$Y$必须对$\beta$有线性关系。
## 误差项零均值
## 同方差性
## 无自相关
## 自变量与误差项不相关
## 无完全多重共线性
## 正态性
# 损失函数
## 种类
误差定义 L1
$$
误差_{i} = \hat{y_{i}}-y_{i}=(w \cdot x_{i} + b)-y_{i}
$$
绝对误差
$$
绝对误差_i =\sum_{i=1}^{m}|\hat{y_i}-y_i|
$$
平均绝对误差MAE
$$
平均绝对误差_i =\frac{1}{m}\sum_{i=1}^{m}|\hat{y_i}-y_i|
$$
平方误差 L2
$$
平方误差_{i} = (\hat{y_{i}}-y_{i})^2=((w \cdot x_{i} + b)-y_{i})^2
$$

均方误差 MSE
平方的目的  去掉负数
2m 乘以2 求2次方程导数的时候进行约分
$$
L(w,b)=\frac{1}{2m} \sum_{i=1}^{m}(\hat{y_i}-y_i)^{2}  
$$
均方根误差 RMSE?  受异常点影响大 因为平方是指数关系 一个离散点的影响将被放到而2次方 所以对结果产生的影响更大
$$
L(w,b)=\sqrt{\frac{1}{2m}\sum_{i=1}^{m}(\hat{y_i}-y_i)^{2} }  
$$
## 最小化损失函数
### 最小二乘法
$$
\min _{w, b} L(w, b)
$$

损失函数以MSE来计算 即
$$
\begin{aligned}
\min_{w, b} L(w, b) 
& = \min_{w,b}{\frac{1}{2m} \sum_{i = 1}^{m}(\hat{y_{i}}-y_i)^{2}} \\\\
& = \min_{w,b}{\frac{1}{2m} \sum_{i = 1}^{m}((w \cdot x_{i} + b)-y_{i})^2} 
\end{aligned}
$$
该函数是一个凸函数（数学上是个凹函数）
所以其一阶导数为0的位置是其最小值 
MSE是一个多元函数  求其一阶导数为0的位置也就是分别对每个自变量分别求偏导使其分别等于0
对$w$求偏导
$$
\begin{align}
\frac{\partial L(w,b)}{\partial w} 
& = \frac{2}{2m}\sum_{i  = 1}^{m}x_i(wx_i+b-y_i) \\\\
& = 2w(\frac{1}{2m}\sum_{i  = 1}^{m}x_i^2) + 2b(\frac{1}{2m}\sum_{i  = 1}^{m}x_i) - 2(\frac{1}{2m}\sum_{i  = 1}^{m}x_iy_i) \\\\
& = w(\frac{1}{m}\sum_{i  = 1}^{m}x_i^2) + b(\frac{1}{m}\sum_{i  = 1}^{m}x_i) - (\frac{1}{m}\sum_{i  = 1}^{m}x_iy_i) \tag{1}
\end{align}
$$
对$b$求偏导（符合函数求倒 使用链式法则）
$$
\begin{align}
\frac{\partial L(w,b)}{\partial b} 
& = \frac{2}{2m}\sum_{i  = 1}^{m}wx_i+b-y_i \\\\
& = 2w(\frac{1}{2m}\sum_{i=1}^{m}x_i) + 2(\frac{1}{2m}\sum_{i=1}^{m}b)-2(\frac{1}{2m}\sum_{i=1}^{m}y_i)\\\\
& = w(\frac{1}{m}\sum_{i=1}^{m}x_i) + b -(\frac{1}{m}\sum_{i=1}^{m}y_i)
\end{align} \tag{2}
$$

令
$$
\begin{align}
\bar{x} &= \frac{1}{m}\sum_{i=1}^{m}x_i  \qquad  \bar{y} = \frac{1}{m}\sum_{i=1}^{m}y_i \\\\
s^{2} &= \frac{1}{m}\sum_{i=1}^{m}x_i^2 \qquad \rho  = \frac{1}{m}\sum_{i=1}^{m}x_iy_i \\\\
\end{align} \tag{3}
$$
这些值都是通过样本数据可以计算出来的

令 式(1) 与式 (2) 均为0
则有
$$
\begin{align}
\frac{\partial L(w,b)}{\partial w} &= 0 \Rightarrow  ws^{2} + b\bar{x} - \rho  = 0   \tag{4} \\\\
\frac{\partial L(w,b)}{\partial b} &= 0 \Rightarrow  w\bar{x} + b - \bar{y}  =0 \tag{5}
\end{align}
$$
讲式（5） 乘以$\bar{x}$ 再与式（4）相减求得$w$
将w 再代回式（5）就可以求出b了
$$
\begin{align}
w & = \frac{\rho -\bar{x}\bar{y}   }{s^{2}-\bar{x}^2 }\tag{6}\\\\
b & = \bar{y} - w\bar{x}  \tag{7}
\end{align}
$$

于是得到了 模型的w以及b的值
协方差样本公式
$$
\begin{align}
\frac{1}{n}\sum_{i  = 1}^{n} (x_i-\bar{x})(y_i-\bar{y}) & = \frac{1}{n}(\sum_{i  = 1}^{n}(x_iy_i-x_i\bar{y}-y_i\bar{x}+ \bar{x}\bar{y})) \\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\sum_{i=1}^{n}x_i\bar{y}-\sum_{i=1}^{n}y_i\bar{x}+\sum_{i=1}^{n}\bar{x}\bar{y})\\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\bar{y}\sum_{i=1}^{n}x_i-\bar{x}\sum_{i=1}^{n}y_i+\sum_{i=1}^{n}\bar{x}\bar{y})\\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i+\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i}{n}+\sum_{i=1}^{n}\bar{x}\bar{y})\\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{2\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i}{n}+\sum_{i=1}^{n}\bar{x}\bar{y})\\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{2\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i}{n}+n\bar{x}\bar{y}) \\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{2\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i}{n}+n\frac{\sum_{i=1}^{n}x_i}{n}\frac{\sum_{i=1}^{n}y_i}{n})  \\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{1}{n}(\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i))\tag{8}
\end{align}
$$

将前面求出来的$w$的值变换变换以下形态
$$
\begin{align}
\rho - \bar{x}\bar{y} & = \frac{1}{m}\sum_{i  = 1}^{m}x_iy_i - \frac{1}{m^2} \sum_{i  = 1}^{m}x_i \sum_{i = 1}^{m}y_i\\\\
&=\frac{1}{m}(\sum_{i  = 1}^{m}x_iy_i - \frac{1}{m} \sum_{i  = 1}^{m}x_i \sum_{i  = 1}^{m}y_i)\\\\
&=\operatorname{Cov}(X,Y) \tag{协方差}
\end{align}
$$
即$w$的分子部分是协方差
那么分母呢
$$
\begin{align}
s^{2} - \bar{x}^2  &= \frac{1}{m}\sum_{i = 1}^{m}x_i^2-(\frac{1}{m}\sum_{i=1}^{m}x_i)^2\qquad \qquad \text{这不就是协方差公式吗}\\\\
&= \frac{1}{m}\sum_{i=1}^{m}(x_i-\bar{x})(x_i-\bar{x})\\\\
&= \frac{1}{m}\sum_{i=1}^{m}(x_i-\bar{x})^2 \qquad \qquad  \text{方差} 
\end{align}
$$
所以$w$的值其实是协方差/方差，即
$$
w=\frac{\operatorname{Cov}(X,Y)}{\sigma^2X}
$$

### 梯度下降法
待完成