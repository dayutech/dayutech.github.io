---
title: 机器学习-线性回归
abbrlink: '19883263'
date: 2025-11-10 16:47:19
tags:
- 机器学习
- 线性回归
- 一元线性回归
- 多元线性回归
- 多项式回归
- 协方差
- 方差
- f检验
- t检验
- 损失函数
- 多重共线性
mathjax: true
top: 1000
categories: 
- - 机器学习
description: 本文简单介绍了机器学回归算法中的线性回归算法，内容包括线性回归的基本假设，一元线性回归的公式推导，多元线性回归的公式推导等内容
---
# 什么是线性回归
线性方程
$$
\hat{y_{i} } = w \cdot x_{i} + b \tag{1}
$$
# 损失函数
## 种类
误差定义 L1
$$
误差_{i} = \hat{y_{i}}-y_{i}=(w \cdot x_{i} + b)-y_{i}
$$
绝对误差
$$
绝对误差_i =\sum_{i=1}^{m}|\hat{y_i}-y_i|
$$
平均绝对误差MAE
$$
平均绝对误差_i =\frac{1}{m}\sum_{i=1}^{m}|\hat{y_i}-y_i|
$$
平方误差 L2
$$
平方误差_{i} = (\hat{y_{i}}-y_{i})^2=((w \cdot x_{i} + b)-y_{i})^2
$$

均方误差 MSE
平方的目的  去掉负数
2m 乘以2 求2次方程导数的时候进行约分
$$
L(w,b)=\frac{1}{2m} \sum_{i=1}^{m}(\hat{y_i}-y_i)^{2}  
$$
均方根误差 RMSE?  受异常点影响大 因为平方是指数关系 一个离散点的影响将被放到而2次方 所以对结果产生的影响更大
$$
L(w,b)=\sqrt{\frac{1}{2m}\sum_{i=1}^{m}(\hat{y_i}-y_i)^{2} }  
$$
## 一元线性回归
### 最小化损失函数
#### 最小二乘法
$$
\min _{w, b} L(w, b)
$$

损失函数以MSE来计算 即
$$
\begin{aligned}
\min_{w, b} L(w, b) 
& = \min_{w,b}{\frac{1}{2m} \sum_{i = 1}^{m}(\hat{y_{i}}-y_i)^{2}} \\\\
& = \min_{w,b}{\frac{1}{2m} \sum_{i = 1}^{m}((w \cdot x_{i} + b)-y_{i})^2} 
\end{aligned}
$$
该函数是一个凸函数（数学上是个凹函数）
所以其一阶导数为0的位置是其最小值 
MSE是一个多元函数  求其一阶导数为0的位置也就是分别对每个自变量分别求偏导使其分别等于0
对$w$求偏导
$$
\begin{align}
\frac{\partial L(w,b)}{\partial w} 
& = \frac{2}{2m}\sum_{i  = 1}^{m}x_i(wx_i+b-y_i) \\\\
& = 2w(\frac{1}{2m}\sum_{i  = 1}^{m}x_i^2) + 2b(\frac{1}{2m}\sum_{i  = 1}^{m}x_i) - 2(\frac{1}{2m}\sum_{i  = 1}^{m}x_iy_i) \\\\
& = w(\frac{1}{m}\sum_{i  = 1}^{m}x_i^2) + b(\frac{1}{m}\sum_{i  = 1}^{m}x_i) - (\frac{1}{m}\sum_{i  = 1}^{m}x_iy_i) \tag{1}
\end{align}
$$
对$b$求偏导（符合函数求倒 使用链式法则）
$$
\begin{align}
\frac{\partial L(w,b)}{\partial b} 
& = \frac{2}{2m}\sum_{i  = 1}^{m}wx_i+b-y_i \\\\
& = 2w(\frac{1}{2m}\sum_{i=1}^{m}x_i) + 2(\frac{1}{2m}\sum_{i=1}^{m}b)-2(\frac{1}{2m}\sum_{i=1}^{m}y_i)\\\\
& = w(\frac{1}{m}\sum_{i=1}^{m}x_i) + b -(\frac{1}{m}\sum_{i=1}^{m}y_i)
\end{align} \tag{2}
$$

令
$$
\begin{align}
\bar{x} &= \frac{1}{m}\sum_{i=1}^{m}x_i  \qquad  \bar{y} = \frac{1}{m}\sum_{i=1}^{m}y_i \\\\
s^{2} &= \frac{1}{m}\sum_{i=1}^{m}x_i^2 \qquad \rho  = \frac{1}{m}\sum_{i=1}^{m}x_iy_i \\\\
\end{align} \tag{3}
$$
这些值都是通过样本数据可以计算出来的

令 式(1) 与式 (2) 均为0
则有
$$
\begin{align}
\frac{\partial L(w,b)}{\partial w} &= 0 \Rightarrow  ws^{2} + b\bar{x} - \rho  = 0   \tag{4} \\\\
\frac{\partial L(w,b)}{\partial b} &= 0 \Rightarrow  w\bar{x} + b - \bar{y}  =0 \tag{5}
\end{align}
$$
讲式（5） 乘以$\bar{x}$ 再与式（4）相减求得$w$
将w 再代回式（5）就可以求出b了
$$
\begin{align}
w & = \frac{\rho -\bar{x}\bar{y}   }{s^{2}-\bar{x}^2 }\tag{6}\\\\
b & = \bar{y} - w\bar{x}  \tag{7}
\end{align}
$$

于是得到了 模型的w以及b的值
协方差样本公式
$$
\begin{align}
\frac{1}{n}\sum_{i  = 1}^{n} (x_i-\bar{x})(y_i-\bar{y}) & = \frac{1}{n}(\sum_{i  = 1}^{n}(x_iy_i-x_i\bar{y}-y_i\bar{x}+ \bar{x}\bar{y})) \\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\sum_{i=1}^{n}x_i\bar{y}-\sum_{i=1}^{n}y_i\bar{x}+\sum_{i=1}^{n}\bar{x}\bar{y})\\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\bar{y}\sum_{i=1}^{n}x_i-\bar{x}\sum_{i=1}^{n}y_i+\sum_{i=1}^{n}\bar{x}\bar{y})\\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i+\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i}{n}+\sum_{i=1}^{n}\bar{x}\bar{y})\\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{2\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i}{n}+\sum_{i=1}^{n}\bar{x}\bar{y})\\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{2\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i}{n}+n\bar{x}\bar{y}) \\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{2\sum_{i=1}^{n}y_i\sum_{i=1}^{n}x_i}{n}+n\frac{\sum_{i=1}^{n}x_i}{n}\frac{\sum_{i=1}^{n}y_i}{n})  \\\\
&=\frac{1}{n}(\sum_{i=1}^{n}x_iy_i-\frac{1}{n}(\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i))\tag{8}
\end{align}
$$

将前面求出来的$w$的值变换变换以下形态
$$
\begin{align}
\rho - \bar{x}\bar{y} & = \frac{1}{m}\sum_{i  = 1}^{m}x_iy_i - \frac{1}{m^2} \sum_{i  = 1}^{m}x_i \sum_{i = 1}^{m}y_i\\\\
&=\frac{1}{m}(\sum_{i  = 1}^{m}x_iy_i - \frac{1}{m} \sum_{i  = 1}^{m}x_i \sum_{i  = 1}^{m}y_i)\\\\
&=\operatorname{Cov}(X,Y) \tag{协方差}
\end{align}
$$
即$w$的分子部分是协方差
那么分母呢
$$
\begin{align}
s^{2} - \bar{x}^2  &= \frac{1}{m}\sum_{i = 1}^{m}x_i^2-(\frac{1}{m}\sum_{i=1}^{m}x_i)^2\qquad \qquad \text{这不就是协方差公式吗}\\\\
&= \frac{1}{m}\sum_{i=1}^{m}(x_i-\bar{x})(x_i-\bar{x})\\\\
&= \frac{1}{m}\sum_{i=1}^{m}(x_i-\bar{x})^2 \qquad \qquad  \text{方差} 
\end{align}
$$
所以$w$的值其实是协方差/方差，即
$$
w=\frac{\operatorname{Cov}(X,Y)}{\sigma^2X}
$$
##### 真实$w$与预测$\hat{w}$的一元推导
直接给出公式
$$
\hat{w} = w + \frac{\sum_{i=1}^{m}(x_i-\bar{x})\varepsilon_i}{\sum_{i=1}^{m}(x_i-\bar{x})^2}\tag{9}
$$
根据前面的推导有
$$
\hat{w}=\frac{\operatorname{Cov}(X,Y)}{\sigma^2X}=\frac{\sum_{i=1}^{m}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{m}(x_i-\bar{x})^2} \tag{10}
$$
在对$w$求偏导数的时候有
$$
\bar{y}=b+w\bar{x_i} \Rightarrow b = \bar{y} - w\bar{x_i}
$$
对真实模型还要加上$\bar{\varepsilon}$
$$
\bar{y}=\bar{b}+\bar{w}x_i+\bar{\varepsilon}
$$
对于真实模型有$y_i=b+wx_i+\varepsilon_i$ 所以有
$$
\sum_{i=1}^{m}(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^{m}(x_i-\bar{x})((b+wx_i+\varepsilon_i)-\bar{y})
$$
所以有
$$
y_i-\bar{y} = b+wx_i+\varepsilon_i-b-w\bar{x_i}-\bar{\varepsilon}=w(x_i-\bar{x})+(\varepsilon_i-\bar{\varepsilon})
$$
所以
$$
\begin{align}
\sum_{i  = 1}^{m}(x_i-\bar{x})(y_i-\bar{y}) & = \sum_{i  = 1}^{m}(x_i-\bar{x})((b+wx_i+\varepsilon_i)-\bar{y})\\\\
&=\sum_{i  = 1}^{m}(x_i-\bar{x})(w(x_i-\bar{x})+(\varepsilon_i-\bar{\varepsilon}))\\\\
&=w\sum_{i  = 1}^{m}(x_i-\bar{x})^2+\sum_{i=1}^{m}(x_i-\bar{x})(\varepsilon_i-\bar{\varepsilon} )\\\\
&=w\sum_{i  = 1}^{m}(x_i-\bar{x})^2+\sum_{i=1}^{m}(x_i-\bar{x})\varepsilon_i-\sum_{i=1}^{m}(x_i-\bar{x})\bar{\varepsilon}\\\\
&=w\sum_{i  = 1}^{m}(x_i-\bar{x})^2+\sum_{i=1}^{m}(x_i-\bar{x})\varepsilon_i-\bar{\varepsilon}\sum_{i=1}^{m}(x_i-\bar{x})\qquad\qquad\text{最后一项的求和部分为0}\\\\
&=w\sum_{i  = 1}^{m}(x_i-\bar{x})^2+\sum_{i=1}^{m}(x_i-\bar{x})\varepsilon_i \tag{11} \\\\
\end{align} 
$$
将式11代回式10
$$
\begin{align}
\hat{w}=\frac{\operatorname{Cov}(X,Y)}{\sigma^2X}&=\frac{\sum_{i=1}^{m}(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^{m}(x_i-\bar{x})^2}\\\\
&=\frac{w\sum_{i  = 1}^{m}(x_i-\bar{x})^2+\sum_{i=1}^{m}(x_i-\bar{x})\varepsilon_i}{\sum_{i=1}^{m}(x_i-\bar{x})^2}\\\\
&=w+ \frac{\sum_{i=1}^{m}(x_i-\bar{x})\varepsilon_i}{\sum_{i=1}^{m}(x_i-\bar{x})^2} 
\end{align}
$$
这就是估计斜率$\hat{w}$与真实斜率$w$之间的精确数学关系。

#### 梯度下降法
有一元线性回归模型
$$
\hat{y_{i} } = w \cdot x_{i} + b
$$
所谓梯度下降法就是在求$w$的时候先假定一个初始的值，比如为$0.06$，然后计算出此时预测值的残差平方和， 然后计算梯度，用初始的$w$减去梯度得到新的$w$，然后在用这个新的$w$计算残差平方和，两者进行比较，取残差平方和更小的那一个$w$
梯度公式
$$
\nabla f(x_p)={f}'(x_p)i 
$$
求新的$w$
$$
w_1=w_0-i\frac{\partial L(w,b)}{\partial w}
$$
梯度下降法在解释变量较多的时候不能有效的通过最小二乘法球的$w$的时候很有用，或者对于一些多项式回归，多元回归的求解中效果较好
**数学原理**
在一元的线性函数，我们使用导数来描述被解释变量随着解释变量变化而变化的快慢，也被称为斜率，当斜率越大的时候这个变化速度就越快，此时我们要向知道$x$向那个方向移动$y$增加的最快，只需要求出当前位置的导数即可，  
沿着导数的方向对$x$进行变化$y$就会得到最快的变化速度。将这个结论或找到多元函数中情况就变得复杂起来，在多元函数中我们也用导数来描述变化率的快慢，这个导数有一个专门的名称方向导数，因为在多元函数中，导数除了有大小之外，  
还有方向，也就是说其是一个向量。
梯度的定义，对函数$f$的各个参数求偏导得到的向量就是$f$的梯度
$$
\nabla f=<\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\frac{\partial f}{\partial x_3},...\frac{\partial f}{\partial x_n} >
$$
将点$p$带入梯度公式得到$p$点的梯度记作$\nabla f\lvert_{p}$
单位向量。单位向量是长度或者模场为1的向量，用来描述方向导数的方向，记作$u$，它各个方向分量的平方和开根号的值为1。例如对单位向量$\mathbf{u}=\langle u_1,u_2,u_3,...u_n\rangle $ 模长记作$\lVert \mathbf{u} \rVert$，也是L2 范数
$$
\lVert \mathbf{u} \rVert=\sqrt{u_1^2+u_2^2+u_3^2+...+u_n^2}
$$
导数的定义
$$
f'(x)=\lim_{\Delta x \to 0} \frac{f(x+\Delta x)-f(x)}{x+\Delta x} 
$$
将这个定义扩展到多元函数就得到了多元函数的方向导数的定义
$$
\begin{align}
D_{u,p}f=\frac{\mathrm{d} f}{\mathrm{d} s}(u,p)&=\lim_{s \to 0}  \frac{f(p_1+su_1,p_2+su_2,...,p_n+su_n)-f(p_1+p_2+...+p_n)}{s}\\\\
&=\lim_{s \to 0}  \frac{f(\textbf{p}+s\textbf{u} )-f(\textbf{p})}{s} 
\end{align}
$$
如果有一个一元一次的线性函数，那么根据导数的定义式，可以推出
$$
f'(x)=\frac{f(x+\Delta x)-f(x)}{x+\Delta x} \Rightarrow f(x+\Delta x) = \Delta xf'(x)+f(x)
$$
如果不是一次的线性函数，那么上面的公式就不成立了，需要在公式的右边部分补加上误差$\varepsilon h$
$$
f(x+\Delta x) = \Delta xf'(x)+f(x) + \varepsilon h
$$
如果是多元函数，就需要将变量换成向量
$$
\begin{align}
f(\textbf{x}+\overrightarrow{\Delta x}) &= \overrightarrow{\Delta x} \cdot<(\frac{\partial f}{\partial x_1}), (\frac{\partial f}{\partial x_2}), (\frac{\partial f}{\partial x_3})... (\frac{\partial f}{\partial x_n}) >+f(\textbf{x}) + \overrightarrow{\varepsilon (\Delta x)}\cdot \overrightarrow{\Delta x} \\\\
&=\overrightarrow{\Delta x} \cdot \nabla f\lvert_x +f(\textbf{x}) + \overrightarrow{\varepsilon (\Delta x)}\cdot \overrightarrow{\Delta x}
\end{align}
$$
将$\overrightarrow{\Delta x}$用 $s\mathbf{u}$替换，$\mathbf{x}$用$\mathbf{p}$替换
$$
f(\mathbf{p}+s\mathbf{u})=s\mathbf{u}\cdot \nabla f\lvert_p + f(\mathbf{p}) + \varepsilon(s\mathbf{u})\cdot s\mathbf{u}
$$
所以
$$
\begin{align}
D_{u,p}f&=\lim_{s \to 0}  \frac{f(\mathbf{p}+s\mathbf{u} )-f(\mathbf{p})}{s} \\\\
&=\lim_{s \to 0}\frac{s\mathbf{u}\cdot \nabla f\lvert_p + f(\mathbf{p}) + \varepsilon(s\mathbf{u})\cdot s\mathbf{u}-f(\mathbf{p})}{s}\\\\
&=\lim_{s \to 0}\mathbf{u}\cdot \nabla f\lvert_p + \varepsilon(s\mathbf{u})\cdot \mathbf{u}\\\\
&=\mathbf{u}\cdot \nabla f\lvert_p=\lVert\mathbf{u}\rVert\lVert\nabla f\lvert_p\rVert\cos \theta 
\end{align}
$$
即**方向导数等于梯度与单位向量的点乘**，当方向向量与梯度同向的时候，也就是$\theta$等于$0$度的时候$D_{u,p}f$取最大值，也就是上升最快，当$\theta$等于$180$度的时候$D_{u,p}f$取最小值，也就是下降最快。
这就是梯度下降法的数学原理，要向误差（残差平方和MSE）下降最快，只需要使得$w$向着梯度相反的方向移动即可。
## 多元线性回归
多元回归的模型式下面这样的 i表示第i个样本 n表示有n个解释变量 w表示权重 x表示解释变量
$$
 \sideset{}{}f_{i,n}(x_{i1},x_{i2},x_{i3}...x_{in})=b\cdot1+w_{21}x_{i2}+w_{31}x_{i3}+w_{41}x_{i4}+...+w_{n1}x_{in}
$$
通过线性代数的概念对上式进行整理
$$
\begin{align}
\textbf{X} &= \begin{pmatrix} 
  & 1 & x_{12} & x_{13} & x_{14} &... &x_{1n} \\\\
  & 1 & x_{22} & x_{23} & x_{24} &... &x_{2n} \\\\
  & 1 & x_{32} & x_{33} & x_{34} &... &x_{3n} \\\\
  & . &. &. &. &. &. &\\\\
  & . &. &. &. &. &. &\\\\
  & . &. &. &. &. &. &\\\\
  & 1 & x_{m2} & x_{m3} & x_{m4} &... &x_{mn}
\end{pmatrix}\qquad\qquad\text{m行 n列}\\\\\\\\
\textbf{W} &=\begin{pmatrix} 
 & b \\\\
 & w_{21}\\\\
 & w_{31}\\\\
 & . \\\\
 & . \\\\
 & . \\\\
 & w_{n1}\\\\
\end{pmatrix}\qquad\qquad\text{n行 1列}\\\\\\\\
\textbf{Y} &= \begin{pmatrix}
&y_{11} \\\\
&y_{21} \\\\
&y_{31} \\\\
&. \\\\
&. \\\\
&. \\\\
&y_{m1} \\\\
\end{pmatrix}\qquad\qquad\text{m行 1列}\\\\\\\\
\textbf{Y}&= \textbf{X}\textbf{W}
\end{align}
$$
根据矩阵的乘法规则最终得到一个$m\times1$的矩阵  每一行都是一个预测值 $\mathbb{R}^{m\times1}=\mathbb{R}^{m\times n}\mathbb{R}^{n\times1}$

# 线性回归的基本假设
## 线性性
线性回归模型的形式为：
$$
Y=b + w_1X_1 +w_2X_2+w_3X_3+...+w_4X_4+\varepsilon 
$$
线性性指的是：模型对参数（即回归系数 $w_1...w_n$是线性的，而不是对自变量 $X$ 必须是线性的。
换句话说：
- 允许$\textbf{X}$出现非线性形式（如$X^2,\ln_{}{x},\sin x$等，这些分线性的X可以被看作一个整体，结果仍然是线性的，后面的多项式回归，注入$X^2$这种也被看作一个整体$X$，那么多项式回归就变成了多元回归），
- 这些非线性变换后的项仍以线性组合的方式出现在模型中。
为什么非得针对$w$必须是线性的，而不是针对$X$是线性的，这是因为在使用最小二乘法计算残差方程的时候的自变量是$w$，而$X$来自样本数据，是已知的常数量，此时$w$才是真正的解释变量  
$w$如果是形如$e ^{w} $类型的非线性形式，那么最小二乘法求解残差方程的最小值的时候就不能利用凸函数的性质直接对两个变量分别求偏导，使其等于零来得到全局的最小值，只能通过梯度下降法来逐渐逼近最小值   
而且还容易陷入到局部最小值的怪圈里，无法求得全局的最小值，即局部最优解。
从这一点出发，我们在解方程的时候其实是解的$w$的值，所以$Y$必须对$w$有线性关系。
## 误差项零均值
在线性性中的线性回归模型$\varepsilon $表示的就是预测值的误差，也叫扰动，代表模型未能捕捉到的其他影响因素。  
误差行零均值说的是，在给定自变量$\textbf{X}$的情况下，误差项 的期望为0，即：
$$
\mathbb{E}[\varepsilon |X]=0
$$
该假设保证了模型的无偏性，说明模型没有系统性地高估或者低估$\textbf{Y}$
线性回归模型$w$地估计量为$\hat{w}$
$$
\hat{w} = w+(\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}\varepsilon \qquad\qquad \text{前面有一元的推导，这里是多元的表达式}
$$
只有当$\mathbb{E}[\varepsilon |X]$为零时，估计量才是无偏的。如果这个假设不成立，模型的回归系数就会偏离系统真实值，比如可能会高估或者低估教育对收入的影响。
违反零均值假设的情况很多，常见因素包括：
- 遗漏其他重要解释变量
- 测量误差
- 联立性/反向因果 $\textbf{X}$与$\textbf{Y}$互相影响
- 样本选择偏差  

**如何理解系统性偏差**
系统性偏差就是对于给定的$X$，模型预测的$\hat{Y}$值总是高于或者低于真实的$Y$的平均水平
扰动项
$$
\varepsilon  = \textbf{Y} - (b + w_1\textbf{X}_1 +w_2\textbf{X}_2+w_3\textbf{X}_3+...+w_4\textbf{X}_4) = \textbf{Y} - \mathbb{E}[\textbf{Y}|\textbf{X}]
$$
所以在给定$\textbf{X}$的情况下$\varepsilon$的条件期望为
$$
\begin{aligned}
\mathbb{E}[\varepsilon|\textbf{X}] &= \mathbb{E}[\textbf{Y}-(b + w_1\textbf{X}_1 +w_2\textbf{X}_2+w_3\textbf{X}_3+...+w_4\textbf{X}_4)|\textbf{X}] \qquad\qquad \text{期望在给定X的时候为常数，向外提}\\\\
&=\mathbb{E}[\textbf{Y}|\textbf{X}] - (b + w_1\textbf{X}_1 +w_2\textbf{X}_2+w_3\textbf{X}_3+...+w_4\textbf{X}_4)
\end{aligned}
$$
根据上式，当且仅当$\mathbb{E}[\varepsilon|\textbf{X}] = 0$的时候有
$$
\mathbb{E}[\textbf{Y}|\textbf{X}] = (b + w_1\textbf{X}_1 +w_2\textbf{X}_2+w_3\textbf{X}_3+...+w_4\textbf{X}_4)
$$
即预测值与期望值无偏差，模型的预测平均值正好等于 Y 在给定 X 下的真实平均值。
但凡$\mathbb{E}[\varepsilon|\textbf{X}] \neq 0$ 就会发生系统性偏差，当然这是不可避免的。

## 同方差性
## 无自相关
## 自变量与误差项不相关
## 无完全多重共线性
## 正态性