---
title: 机器学习-线性回归
abbrlink: '19883263'
date: 2025-11-10 16:47:19
tags:
mathjax: true
top: 1000
categories: 
- - 机器学习
---
# 什么是线性回归
线性方程
$$
\hat{y_{i} } = w \cdot x_{i} + b \tag{1}
$$
# 损失函数
## 种类
误差定义 L1
$$
误差_{i} = \hat{y_{i}}-y_{i}=(w \cdot x_{i} + b)-y_{i}
$$

平方误差 L2
$$
平方误差_{i} = (\hat{y_{i}}-y_{i})^2=((w \cdot x_{i} + b)-y_{i})^2
$$

均方误差 MSE
平方的目的  1、 去掉负数 2、 方便约分
$$
L(w,b)=\frac{1}{2m} \sum_{i=1}^{m}(\hat{y_i}-y_i)^{2}  
$$
均方根误差 RMSE?  受异常点影响大 因为平方是指数关系 一个离散点的影响将被放到而2次方 所以对结果产生的影响更大
$$
L(w,b)=\frac{1}{2m} \sqrt{\sum_{i=1}^{m}(\hat{y_i}-y_i)^{2} }  
$$
## 最小化损失函数
### 最小二乘法
$$
\min _{w, b} L(w, b)
$$

损失函数以MSE来计算 即
$$
\begin{aligned}
\min_{w, b} L(w, b) 
& = \min_{w,b}{\frac{1}{2m} \sum_{i = 1}^{m}(\hat{y_{i}}-y_i)^{2}} \\\\
& = \min_{w,b}{\frac{1}{2m} \sum_{i = 1}^{m}((w \cdot x_{i} + b)-y_{i})^2} 
\end{aligned}
$$
该函数是一个凸函数（数学上是个凹函数）
所以其一阶导数为0的位置是其最小值 
MSE是一个多元函数  求其一阶导数为0的位置也就是分别对每个自变量分别求偏导使其分别等于0
对$w$求偏导
$$
\begin{align}
\frac{\partial L(w,b)}{\partial w} 
& = \frac{2}{2m}\sum_{i  = 1}^{m}x_i(wx_i+b-y_i) \\\\
& = 2w(\frac{1}{2m}\sum_{i  = 1}^{m}x_i^2) + 2b(\frac{1}{2m}\sum_{i  = 1}^{m}x_i) - 2(\frac{1}{2m}\sum_{i  = 1}^{m}x_iy_i) \\\\
& = w(\frac{1}{m}\sum_{i  = 1}^{m}x_i^2) + b(\frac{1}{m}\sum_{i  = 1}^{m}x_i) - (\frac{1}{m}\sum_{i  = 1}^{m}x_iy_i) \tag{1}
\end{align}
$$
对$b$求偏导（符合函数求倒 使用链式法则）
$$
\begin{align}
\frac{\partial L(w,b)}{\partial b} 
& = \frac{2}{2m}\sum_{i  = 1}^{m}wx_i+b-y_i \\\\
& = 2w(\frac{1}{2m}\sum_{i=1}^{m}x_i) + 2(\frac{1}{2m}\sum_{i=1}^{m}b)-2(\frac{1}{2m}\sum_{i=1}^{m}y_i)\\\\
& = w(\frac{1}{m}\sum_{i=1}^{m}x_i) + b -(\frac{1}{m}\sum_{i=1}^{m}y_i)
\end{align} \tag{2}
$$

令
$$
\begin{align}
\bar{x} &= \frac{1}{m}\sum_{i=1}^{m}x_i  \qquad  \bar{y} = \frac{1}{m}\sum_{i=1}^{m}y_i \\\\
s^{2} &= \frac{1}{m}\sum_{i=1}^{m}x_i^2 \qquad\rho  = \frac{1}{m}\sum_{i=1}^{m}x_iy_i \\\\
\end{align} \tag{3}
$$
这些值都是通过样本数据可以计算出来的

令 式(1) 与式 (2) 均为0
则有
$$
\begin{align}
\frac{\partial L(w,b)}{\partial w} &= 0 \Rightarrow  ws^{2} + b\bar{x} - \rho  = 0   \tag{4} \\\\
\frac{\partial L(w,b)}{\partial b} &= 0 \Rightarrow  w\bar{x} + b - \bar{y}  =0 \tag{5}
\end{align}
$$
讲式（5） 乘以$\bar{x}$ 再与式（4）相减求得$w$
将w 再代回式（5）就可以求出b了
$$
\begin{align}
w & = \frac{\bar{x}\bar{y} + \rho   }{s^{2}-\bar{x}^2 }\\\\
b & = \bar{y} - m\bar{x}  
\end{align}
$$

于是得到了 模型的w以及b的值
### 梯度下降法
待完成